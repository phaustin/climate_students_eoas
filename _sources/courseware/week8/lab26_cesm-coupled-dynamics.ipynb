{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b64cbbb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(nb:coupdyn)=\n",
    "# Coupled Dynamics in the CESM\n",
    "\n",
    "This notebook is an extension of [The Climate Laboratory](https://brian-rose.github.io/ClimateLaboratoryBook) by [Brian E. J. Rose](http://www.atmos.albany.edu/facstaff/brose/index.html), University at Albany. Notebook by Rachel H. White, University of British Columbia (https://www.eoas.ubc.ca/people/rachelwhite)\n",
    "\n",
    "There are 'Discussion points' and 'Exercises' throughout these notebooks. You should come to class prepared to discuss your thoughts on the Discussion points.\n",
    "\n",
    "Learning goals:\n",
    "- Be able to analyse coupled (atmosphere-ocean) dynamics in the CESM climate model data\n",
    "- Understand how the coupled system produces low-frequency climate variability, and consider the implications of this for understanding climate change\n",
    "- Evaluate differences in low-frequency climate variability between slab-ocean and fully coupled simulations\n",
    "     \n",
    "**You need to be connected to the internet to run the code in this notebook**\n",
    "\n",
    "You can browse the available data through a web interface here:\n",
    "\n",
    "http://thredds.atmos.albany.edu:8080/thredds/catalog.html\n",
    "\n",
    "Within this folder called `CESM archive`, you will find another folder called `som_input` which contains all the input files.\n",
    "\n",
    "________\n",
    "## Low frequency variability in the CESM\n",
    "________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e99e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import xarray as xr\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import Ngl\n",
    "import cartopy\n",
    "import cartopy.util\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from eofs.standard import Eof\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "cesm_data_path = \"http://thredds.atmos.albany.edu:8080/thredds/dodsC/CESMA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in slab ocean data\n",
    "atmfile = xr.open_dataset( cesm_data_path + 'som_1850_f19/concatenated/' + 'som_1850_f19.cam.h0.nc')\n",
    "#atmfile = xr.open_dataset( cesm_data_path + \"cpl_1850_f19/concatenated/cpl_1850_f19.cam.h0.nc\")\n",
    "atmfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06582258",
   "metadata": {},
   "source": [
    "This data is a concatenated dataset of monthly data - we expect that it's monthly data because of the file name - with the CESM model the 'h0' in the file name is short for 'history file 0'. The default is for 'history file 0' to be monthly data. You can change the CESM model to output additional history files (h1, h2, h3...) at a different time resolution. The default for 'h1' is daily data. If you are interested in looking at storm tracks, and their influence on heat transport and surface weather, you might have h1 be outputting daily data for just a few variables of interest to eddy dynamics (winds, geopotential height), and h2 to be outputting 6-hourly data for a few 2D fields such as precipitation, surface temperature and surface pressure (outputting 6-hourly data on 3D fields will take up a lot of space very quickly).\n",
    "\n",
    "We can confirm that it is monthly data by looking at the times in the file above. Compare this to the h1 files that are also available from this thredds directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc03c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "atmfileh1_0001 = xr.open_dataset(cesm_data_path + 'som_1850_f19/atm/hist/' + \n",
    "                                 'som_1850_f19.cam.h1.0001-01-01-00000.nc')\n",
    "#atmfile = xr.open_dataset( cesm_data_path + \"cpl_1850_f19/concatenated/cpl_1850_f19.cam.h0.nc\")\n",
    "atmfileh1_0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2bc041",
   "metadata": {},
   "source": [
    "**Discussion:** _What is the time resolution of the h1 files?_\n",
    "\n",
    "As discussed in Hartmann chapter 8, single point correlation maps are one way of looking at the variability - these show how values of a variables at places all around the globe vary together with that variable at a particular point; that is, they show spatial patterns of variability. Let's try to reproduce figure 8.4b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c02c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate new arrays that hold seasonal data\n",
    "Z3_seas = {}\n",
    "for label,data in atmfile.Z3.groupby('time.season'):\n",
    "    Z3_seas[label] = data\n",
    "    \n",
    "PS_seas = {}\n",
    "for label,data in atmfile.PS.groupby('time.season'):\n",
    "    PS_seas[label] = data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b08cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we want to convert to pressure levels \n",
    "def hybrid2pres(filein,varin,PSin,plevels,timedim=True):\n",
    "    # the parameters hyam and hybm are parameters used to calculate the pressure from the level value\n",
    "    hyam = filein['hyam']\n",
    "    hybm = filein['hybm']\n",
    "    p0mb = filein['P0']/100.0\n",
    "    \n",
    "    varpres = Ngl.vinth2p(varin, hyam, hybm, plevels, PSin, 1, p0mb, 1, False)\n",
    "    # This has set values below the surface to 1E30. We want these to be nan:\n",
    "    varpres = np.where(varpres>=1E30,np.nan,varpres)\n",
    "\n",
    "    # Create xarray from this numpy ndarray:\n",
    "    if timedim:\n",
    "        varpres_xr = xr.DataArray(varpres,\n",
    "                              dims=['time','plev','lat','lon'],\n",
    "                              coords={'time':varin.time,'plev':plevels,\n",
    "                                      'lat':varin.lat,'lon':varin.lon})\n",
    "    else:\n",
    "        varpres_xr = xr.DataArray(varpres,\n",
    "                              dims=['plev','lat','lon'],\n",
    "                              coords={'plev':plevels,\n",
    "                                      'lat':varin.lat,'lon':varin.lon})\n",
    "        \n",
    "    return(varpres_xr)\n",
    "\n",
    "# we're interested in the 500mb level, so we can just select that level \n",
    "# For DJF:\n",
    "Z500_DJF = hybrid2pres(atmfile,Z3_seas['DJF'],PS_seas['DJF'],[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a091ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the point that we want to calculate correlations from: (fig. 8.4c)\n",
    "# Estimate that P is at: 45N, 200E (\n",
    "# Warning - when using new datasets you should always check whether the\n",
    "# longitudes are 0-360, not -180 to 180)\n",
    "\n",
    "# Because the latitudes are not spaced such that there is a grid box centred on 45N, if\n",
    "# we use .sel(lat=45) it will fail:\n",
    "\n",
    "# pointvals = Z500_DJF.sel(lat=45).sel(lon=200)\n",
    "\n",
    "# We can either look at the data and select a latitude value that does exist,\n",
    "# or ask xarray to just pick the nearest point to the value we have asked for:\n",
    "pointvals = Z500_DJF.sel(lat=45,method='nearest').sel(lon=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f795533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now correlate this timeseries with that from a nearby gridpoint:\n",
    "tocorr = Z500_DJF.sel(lat=45,method='nearest').sel(lon=210)\n",
    "#print(pointvals.squeeze())\n",
    "corr,r = sp.stats.pearsonr(pointvals.squeeze(),tocorr.squeeze())\n",
    "\n",
    "print(corr,r)\n",
    "\n",
    "# We can see there is a very strong correlation, that has a very low p-value (r; note\n",
    "# this assumes your data are normally distrubuted which may not be the case!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build the 1-point correlation maps we can repeat this for every point\n",
    "nlats = len(Z500_DJF.lat)\n",
    "nlons = len(Z500_DJF.lon)\n",
    "\n",
    "corrarray = np.zeros([nlats,nlons],float)\n",
    "rarray = np.zeros([nlats,nlons],float)\n",
    "\n",
    "for ilat in range(0,nlats):\n",
    "    for ilon in range(0,nlons):\n",
    "        # if we use isel instead of sel, we select on indices, instead of values:\n",
    "        tocorr = Z500_DJF.isel(lat=ilat).isel(lon=ilon)\n",
    "        corrarray[ilat,ilon],rarray[ilat,ilon] = sp.stats.pearsonr(pointvals.squeeze(),tocorr.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846bcc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now plot this 1-point correlation array for comparison with fig. 8.4c in Hartmann\n",
    "toplot = corrarray\n",
    "ncols=1\n",
    "nrows=1\n",
    "n=1\n",
    "\n",
    "title='DJF 1-point correlation map Z500 monthly data'\n",
    "\n",
    "proj=ccrs.Orthographic(central_latitude=90)\n",
    "ax = plt.subplot(ncols,nrows,n,projection=proj)\n",
    "ax.coastlines()\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree())\n",
    "\n",
    "cp = plt.contourf(Z500_DJF.lon,Z500_DJF.lat,toplot,levels=np.arange(-1,1.01,0.2),extend='both',\n",
    "                  transform=ccrs.PlateCarree(),cmap='RdBu_r')\n",
    "\n",
    "# Add colorbar\n",
    "cb = plt.colorbar(cp)\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e64f62",
   "metadata": {},
   "source": [
    "It is often useful to mask your data for statistical significance, and this can be done fairly easily with python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b9afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now plot this 1-point correlation array for comparison with fig. 8.4c in Hartmann\n",
    "\n",
    "# Set the p-value for which we want to show the data:\n",
    "p=0.05\n",
    "\n",
    "toplot = corrarray\n",
    "mask = np.where(rarray<p,1.0,0.0)\n",
    "\n",
    "ncols=1\n",
    "nrows=1\n",
    "n=1\n",
    "\n",
    "title='DJF 1-point correlation map Z500 monthly data'\n",
    "\n",
    "proj=ccrs.Orthographic(central_latitude=90)\n",
    "ax = plt.subplot(nrows,ncols,n,projection=proj)\n",
    "ax.coastlines()\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree())\n",
    "\n",
    "cp = plt.contourf(Z500_DJF.lon,Z500_DJF.lat,toplot,levels=np.arange(-1,1.01,0.2),extend='both',\n",
    "                  transform=ccrs.PlateCarree(),cmap='RdBu_r')\n",
    "cb = plt.colorbar(cp)\n",
    "\n",
    "# Choose to hatch where the data is NOT statistically significant at our chosen level\n",
    "cp = plt.contourf(Z500_DJF.lon,Z500_DJF.lat,mask,levels=[0,0.99],hatches=['/',None],\n",
    "                  transform=ccrs.PlateCarree(),colors='none')\n",
    "\n",
    "plt.title(title + '; p<' + str(p))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed433a",
   "metadata": {},
   "source": [
    "**Exercise:** _Repeat this analysis for a 1-point correlation for point A on Figure 8.4d. Add the letters A and P onto\n",
    "these figures in the correct places to illustrate the point used for the correlations._\n",
    "##Discussion:** _Why are we only looking at figure 8.4c and 8.4d. What are the additional steps required to reproduce\n",
    "figures 8.4a and b?\n",
    "\n",
    "## Slab ocean and coupled ocean low frequency variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in fully coupled ocean data\n",
    "atmfile_cpl = xr.open_dataset( cesm_data_path + 'cpl_1850_f19/concatenated/' + 'cpl_1850_f19.cam.h0.nc')\n",
    "#atmfile = xr.open_dataset( cesm_data_path + \"cpl_1850_f19/concatenated/cpl_1850_f19.cam.h0.nc\")\n",
    "atmfile_cpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc419c",
   "metadata": {},
   "source": [
    "**Exercise:** _Calculate the single point correlation maps for the coupled data, and compare to the slab ocean plots._\n",
    "**Discussion:** _Are these plots significantly different? Is this what you would expect? Why/why not?_\n",
    "\n",
    "## The El Nino Southern Oscillation\n",
    "\n",
    "If you are not familiar with the El Nino Southern Oscillation (ENSO) I suggest you read through section 8.3 of the Hartmann book before attempting this section.\n",
    "\n",
    "First, let's check that the model correctly simulates the average zonally asymmetric circulation patterns in the tropics (for comparison with figure 8.9). \n",
    "\n",
    "OMEGA is the variable containing the vertical (pressure) velocity, in Pa/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee06d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because there isn't a grid box centred on 0, we take an average across the equator. We first check that\n",
    "# this is symmetrical about the equator:\n",
    "print(atmfile.sel(lat=slice(-2,2)).lat)\n",
    "\n",
    "omega = atmfile.OMEGA.sel(lat=slice(-2,2)).mean(dim='time')\n",
    "PS = atmfile.PS.sel(lat=slice(-2,2)).mean(dim='time')\n",
    "omega_cpl = atmfile_cpl.OMEGA.sel(lat=slice(-2,2)).mean(dim='time')\n",
    "PS_cpl = atmfile_cpl.PS.sel(lat=slice(-2,2)).mean(dim='time')\n",
    "\n",
    "# convert to pressure levels\n",
    "pnew = [1000.,850.,700.,600.,500.,400.,300.,250.,200.,150.,100.,70.,50.,30.,20.,10.,5.]\n",
    "\n",
    "omega_pres = hybrid2pres(atmfile,omega,PS,pnew,timedim=False).mean(dim='lat')\n",
    "omega_cpl_pres = hybrid2pres(atmfile_cpl,omega_cpl,PS_cpl,pnew,timedim=False).mean(dim='lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd006a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the equatorial cross-section\n",
    "omega_pres.plot.contour(levels = np.arange(0,0.1,0.02),colors='r')\n",
    "omega_pres.plot.contour(levels = np.arange(-0.1,0,0.02),colors='b',linestyles='-')\n",
    "plt.yscale('log')\n",
    "# invert the axis so it represents height, but shows pressure\n",
    "plt.gca().invert_yaxis()\n",
    "# set the top and bottom pressure of the plot\n",
    "plt.ylim(1000,100)\n",
    "plt.title('Slab ocean equatorial vertical pressure velocity, annual mean')\n",
    "plt.show()\n",
    "\n",
    "omega_cpl_pres.plot.contour(levels = np.arange(0,0.1,0.02),colors='r')\n",
    "omega_cpl_pres.plot.contour(levels = np.arange(-0.1,0,0.02),colors='b',linestyles='-')\n",
    "plt.yscale('log')\n",
    "# invert the axis so it represents height, but shows pressure\n",
    "plt.gca().invert_yaxis()\n",
    "# set the top and bottom pressure of the plot\n",
    "plt.ylim(1000,100)\n",
    "plt.title('Coupled ocean equatorial vertical pressure velocity, annual mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85af0bcb",
   "metadata": {},
   "source": [
    "**Discussion:** _Compare the model results to the observations shown in the Hartmann textbook and to each other._\n",
    "\n",
    "Now let's calculate indices of the ENSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa1fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can calculate the Nino3 index in these models and compare the variability.\n",
    "# Convert from K to C on the way\n",
    "Nino3 = atmfile.TS.sel(lat=slice(-5,5),lon=slice(210,270)).mean(dim=['lat','lon']) - 273.15\n",
    "Nino3_cpl = atmfile_cpl.TS.sel(lat=slice(-5,5),lon=slice(210,270)).mean(dim=['lat','lon']) - 273.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b816bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate climatologies and compare\n",
    "def create_clim(indata):\n",
    "    nyears = int(len(indata.time)/12)\n",
    "    # reshape data\n",
    "    indata_years = np.reshape(indata.values,(nyears,12))\n",
    "    clim = indata_years.mean(axis=0)\n",
    "    return(clim)\n",
    "\n",
    "Nino3_clim = create_clim(Nino3)\n",
    "Nino3_cpl_clim = create_clim(Nino3_cpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to compare the Nino3 climatologies of the two models.\n",
    "plt.plot(np.arange(1,13),Nino3_clim,label='slab ocean',color='k')\n",
    "plt.plot(np.arange(1,13),Nino3_cpl_clim,label='coupled ocean',color='b')\n",
    "plt.legend()\n",
    "plt.xlabel('month of year')\n",
    "plt.ylabel('Temperature (C)')\n",
    "plt.title('Nino3 climatology')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d94823",
   "metadata": {},
   "source": [
    "We can see that both models follow a similar climatology in the Nino3 index, which is largely driven by the seasonal cycle. It makes sense that both oceans reproduce the seasonal cycle in surface temperature as this is, at least in tropics, strongly forced by incoming solar radiation. Note that this seasonal cycle shows two peaks (Dec/Jan and May) and two troughs (Feb and Sep) per year.\n",
    "\n",
    "\n",
    "**Discussion:** _Why does the climatology of tropical SSTs show two peaks per year? Is that what you would expect to see in observations, or do you think there is something wrong with this model?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29327350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the anomalies from this climatology:\n",
    "# need to repeat the climatology for each year in order to subtract arrays\n",
    "nyears = 30\n",
    "nyears_cpl=20\n",
    "Nino3_clim_all = np.tile(Nino3_clim,nyears)\n",
    "Nino3_anoms = Nino3 - Nino3_clim_all\n",
    "\n",
    "Nino3_cpl_clim_all = np.tile(Nino3_cpl_clim,nyears_cpl)\n",
    "Nino3_cpl_anoms = Nino3_cpl - Nino3_cpl_clim_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824b341",
   "metadata": {},
   "source": [
    "**Exercise:** _Create a plot to compare the anomalies in the slab ocean and fully coupled ocean experiments. Note that the coupled ocean has only 20 years, while the slab ocean has 30._\n",
    "\n",
    "\n",
    "**Discussion:** _Which model has more variability? Is this what you would expect? Which model has more low frequency variability? What does this tell you about variability of ocean surface tempertures in the Nino3 region?_\n",
    "\n",
    "\n",
    "Rather than estimating the variability, we can calculate the power specturm using fast fourier transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a simple (non-normalized) power density spectrum using Fourier analysis\n",
    "Nino3_fft = np.fft.rfft(Nino3_anoms)\n",
    "\n",
    "Nino3_powerspec = np.square(np.abs(Nino3_fft))\n",
    "sampling_rate = 12 # cycles per year\n",
    "frequency = np.linspace(0, sampling_rate/2, len(Nino3_powerspec))\n",
    "\n",
    "plt.plot(frequency,Nino3_powerspec,color='k',label='slab ocean')\n",
    "\n",
    "# Repeat for coupled experiment\n",
    "Nino3_cpl_fft = np.fft.rfft(Nino3_cpl_anoms)\n",
    "\n",
    "Nino3_cpl_powerspec = np.square(np.abs(Nino3_cpl_fft))\n",
    "sampling_rate = 12 # cycles per year\n",
    "frequency = np.linspace(0, sampling_rate/2, len(Nino3_cpl_powerspec))\n",
    "\n",
    "plt.plot(frequency,Nino3_cpl_powerspec,color='b',label='coupled ocean')\n",
    "\n",
    "plt.ylim(0,20000)\n",
    "plt.xlim(0,2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e365127",
   "metadata": {},
   "source": [
    "**Discussion:** _Do the two models simulate any realistic ENSO variance? What are the differences between the 2 spectra shown above and between that shown in Fig. 8.12 in the Hartmann book for the observations? Think about why these differences might come about, and what they tell us about the models?_\n",
    "\n",
    "If we want a more detailed spectra for the coupled ocean we need more than 20 years. You can find a simulation of 80 years with CO2 ramping here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a623b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "atmfile_cpl_ramp = xr.open_dataset( cesm_data_path + \n",
    "                    'cpl_CO2ramp_f19/concatenated/' + 'cpl_CO2ramp_f19.cam.h0.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca531a16",
   "metadata": {},
   "source": [
    "This starts off at year 20 of the cpl_1850_f19 simulation, and ramps up CO2, similar to the real World, which gives us a test of climate change as well. This means we also need to remove a linear trend from the TS data before calculating the power spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nino3_cpl_ramp = (atmfile_cpl_ramp.TS\n",
    "                      .sel(lat=slice(-5,5),lon=slice(210,270))\n",
    "                      .mean(dim=['lat','lon']) - 273.15)\n",
    "Nino3_cpl_ramp_clim = create_clim(Nino3_cpl_ramp)\n",
    "\n",
    "nyears_cpl_ramp=80\n",
    "Nino3_cpl_anoms = Nino3_cpl_ramp - np.tile(Nino3_cpl_ramp_clim,nyears_cpl_ramp)\n",
    "\n",
    "# Calculate the linear slope using regression:\n",
    "x = np.arange(0,80,1/12)\n",
    "regress = sp.stats.linregress(x,Nino3_cpl_anoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2813a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot anomalies and linear regression\n",
    "plt.plot(x,Nino3_cpl_anoms)\n",
    "plt.plot(x,regress.slope*x + regress.intercept,color='k')\n",
    "plt.show()\n",
    "\n",
    "# Now remove this linear slope\n",
    "Nino3_cpl_anoms_detrend = Nino3_cpl_anoms - regress.slope*x + regress.intercept\n",
    "plt.plot(x,Nino3_cpl_anoms_detrend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ab140a",
   "metadata": {},
   "source": [
    "**Exercise:** _Calculate and plot the power spectrum for these de-trended data for comparison with the 20 year dataset and the observations in the Hartmann book._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f919e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a simple (non-normalized) power density spectrum using Fourier analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b336b81",
   "metadata": {},
   "source": [
    "## ENSO and the PDO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e6d14c",
   "metadata": {},
   "source": [
    "We can now look at the spatial signal of the ENSO using Empirical Orthogonal Functions (EOFs), a form of Principal Component Analysis. This webpage: https://climatedataguide.ucar.edu/climate-data-tools-and-analysis/empirical-orthogonal-function-eof-analysis-and-rotated-eof-analysis provides a brief overview of EOFs for those not familiar with them. For those who are familiar with them, there is a key sentence here:\n",
    "**EOF analysis is _not_ based on physical principles**. This means that just because you find an EOF pattern in your data does NOT mean there is necessarily a physical process that pattern represents. EOFs are a useful tool, but need to be combined with physical understanding.\n",
    "For a more in-depth review, see: https://rmets.onlinelibrary.wiley.com/doi/epdf/10.1002/joc.1499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a899c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "indata=atmfile_cpl_ramp.TS - 273.15\n",
    "\n",
    "# Mask over land values\n",
    "landmask = np.where(atmfile_cpl_ramp.LANDFRAC>0,np.nan,1)\n",
    "\n",
    "SST = indata * landmask\n",
    "\n",
    "nlons = len(SST.lon)\n",
    "nlats = len(SST.lat)\n",
    "nyears_cpl_ramp = int(len(SST.time)/12)\n",
    "\n",
    "monthly = np.reshape(indata.values,(nyears_cpl_ramp,12,nlats,nlons))\n",
    "clim = monthly.mean(axis=0)\n",
    "print(clim.shape)\n",
    "\n",
    "global_SST_anoms = SST - np.tile(clim,[nyears_cpl_ramp,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46873f",
   "metadata": {},
   "source": [
    "**Note: we are using surface temperature from the atmospheric files, rather than the surface temperature from the ocean files: this is because the ocean data are on a different grid, that would require more complex re-gridding**\n",
    "\n",
    "\n",
    "We are going to calculate EOFs using the EOF package written by Andrew Dawson: https://github.com/ajdawson/eofs\n",
    "\n",
    "I recommend trying to follow the example here: https://github.com/ajdawson/eofs/blob/master/examples/standard/sst_example.py to find the ENSO signal in our temperature anomalies. You will have to make some changes due to changes in syntax from older version of python.\n",
    "\n",
    "If you get stuck, have a look at the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39b2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ac144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an EOF solver to do the EOF analysis. Square-root of cosine of\n",
    "# latitude weights are applied before the computation of EOFs.\n",
    "coslat = np.cos(np.deg2rad(global_SST_anoms.lat))\n",
    "wgts = np.sqrt(coslat)\n",
    "wgts_tile = np.transpose(np.tile(wgts,(nlons,1)))\n",
    "\n",
    "# Check weights:\n",
    "plt.contourf(global_SST_anoms.lon,global_SST_anoms.lat,wgts_tile)\n",
    "plt.colorbar()\n",
    "# Calculate EOFs\n",
    "solver = Eof(global_SST_anoms.values, weights=wgts_tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015572ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "eof1 = solver.eofsAsCorrelation(neofs=1)\n",
    "pc1 = solver.pcs(npcs=1, pcscaling=1)\n",
    "\n",
    "# Plot the leading EOF expressed as correlation in the Pacific domain.\n",
    "clevs = np.linspace(-1, 1, 11)\n",
    "ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=190))\n",
    "fill = ax.contourf(global_SST_anoms.lon, global_SST_anoms.lat, eof1.squeeze(), clevs,\n",
    "                   transform=ccrs.PlateCarree(), cmap=plt.cm.RdBu_r)\n",
    "ax.add_feature(cfeature.LAND, facecolor='w', edgecolor='k')\n",
    "cb = plt.colorbar(fill, orientation='horizontal')\n",
    "cb.set_label('correlation coefficient', fontsize=12)\n",
    "plt.title('EOF1 expressed as correlation', fontsize=16)\n",
    "\n",
    "# Plot the leading PC time series.\n",
    "plt.figure()\n",
    "months = range(1, 12*80+1)\n",
    "plt.plot(months, pc1, color='b', linewidth=2)\n",
    "plt.axhline(0, color='k')\n",
    "plt.title('PC1 Time Series')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Normalized Units')\n",
    "plt.ylim(-3, 3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb031b04",
   "metadata": {},
   "source": [
    "**Discussion:** _What is the main pattern that we see in the first EOF? Is this the ENSO signal? If not, what is it?_\n",
    "\n",
    "**Exercise:** _Repeat the analysis above with the extra step required to get the ENSO signal as the first EOF_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdbe1df",
   "metadata": {},
   "source": [
    "**Exercise:** _We have already seen that the power spectrum of the Nino3 variability in the slab ocean model does not match observations. Have a look at the first EOF of SST in the slab ocean model. What does this tell you about the spatial distribution of variability in the slab ocean model?_\n",
    "\n",
    "\n",
    "### Further Exploration\n",
    "**Open-ended exercise:** _Read section 8.4 of the Hartmann book. Use the tools provided in this notebook to create one or two more plots exploring the variability in the CESM data. You could:_\n",
    "- look at the effect of CO2 on the ENSO spectra and spatial distribution by comparing the first and last years of the cpl_CO2_ramp experiment. You could also look at the CO2 rampdown experiment in the Thredds catalogue\n",
    "- Look at variability in different regions, investigating, for example, the PDO, or the AMO.\n",
    "- Look at differences between in ENSO, PDO, AMO characteristics for different 30 year chunks of the 80 year simulations - what can this teach you about our confidence of low frequency variability from relatively short observation records?\n",
    "\n",
    "\n",
    "\n",
    "To better understand low-frequency variability, particularly multi-decadal variability, we need more data than we have from observations. One way to get these data is by running ensembles of climate models. This is the theme of the next notebook, which will introduce you to the CESM Large Ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754451ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.9.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   40,
   57,
   61,
   66,
   72,
   77,
   83,
   95,
   126,
   142,
   154,
   169,
   190,
   194,
   224,
   233,
   238,
   251,
   268,
   289,
   295,
   302,
   315,
   324,
   331,
   341,
   351,
   374,
   380,
   383,
   387,
   401,
   411,
   415,
   417,
   421,
   427,
   444,
   455,
   459,
   473,
   498,
   504,
   519
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}