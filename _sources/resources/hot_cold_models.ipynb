{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f435f8",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "(resource:hot_cold)=\n",
    "# Worksheet: Comparing total cloud fraction in hot and cold models\n",
    "\n",
    "This notebook is motivated by  [Zelinka et al., 2020](https://www.dropbox.com/s/2mbf87tdviosdki/Geophysical%20Research%20Letters%20-%202020%20-%20Zelinka%20-%20Causes%20of%20Higher%20Climate%20Sensitivity%20in%20CMIP6%20Models.pdf?dl=0) which includes\n",
    "a  supplementary [table S1](https://www.dropbox.com/s/5l7kmf2rxhflgpc/zelinka_grl_supplement.pdf?dl=0) that\n",
    "lists the equilibrium climate senitivity for 27 CMIP6 models.\n",
    "\n",
    "They find that differences in southern ocean low cloud amount can explain much of the climate\n",
    "sensitivity differences.  Below we ask you to compare southern ocean cloud fraction for \n",
    "a high and low climate sensitivity model run.\n",
    "\n",
    "Below we compare the total cloud fraction [clt](https://cmip-publications.llnl.gov/view/CMIP6/?type=variable&option=clt%20-%20Total%20Cloud%20Cover%20Percentage)\n",
    "for CESM2 (NCAR) and MIROC6 (Japan)\n",
    "\n",
    "\n",
    "\n",
    "Things to note in this notebook\n",
    "\n",
    "- {ref}`sec:lazy`\n",
    "- {ref}`sec:isel`\n",
    "- {ref}`sec:checkpoint`\n",
    "- {ref}`sec:weighted`\n",
    "\n",
    "\n",
    "Bottom line:  These two models have very different ideas about the cloud fraction over the Southern Ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702923d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "import xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c7841f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Load the catalog\n",
    "\n",
    "At import time, intake-esm plugin is available in intakeâ€™s registry as\n",
    "`esm_datastore` and can be accessed with `intake.open_esm_datastore()` function.\n",
    "Use the `intake_esm.tutorial.get_url()` method to access smaller subsetted catalogs for tutorial purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake_esm\n",
    "# url =\"https://raw.githubusercontent.com/NCAR/intake-esm-datastore/main/catalogs/pangeo-cmip6.json\"\n",
    "url_official = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6103f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = intake.open_esm_datastore(url_official)\n",
    "cat.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c58d5",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The summary above tells us that this catalog contains 514818 data assets.\n",
    "We can get more information on the individual data assets contained in the\n",
    "catalog by looking at the underlying pandas dataframe created when we load the catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c292c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055cc99f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The first data asset listed in the catalog contains:\n",
    "\n",
    "- the surace pressure (variable_id='ps'), as a function of latitude, longitude, time,\n",
    "\n",
    "- the high resolution version of the CMCC climate model (source_id='CMCC-CM2-HR4'),\n",
    "\n",
    "- the high resolution model intercomparison expermenet (experiment_id='HighResMIP'),\n",
    "\n",
    "- developed by the Euro-Mediterranean Centre on Climate Change (instution_id='CMCC'),\n",
    "\n",
    "- run as part of the Coupled Model Intercomparison Project (activity_id='CMIP')\n",
    "\n",
    "And is located in Google Cloud Storage at 'gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/highresSST-present/r1i1p1f1/Amon/ps/gn/v20170706/\"\n",
    "\n",
    "## Finding unique entries\n",
    "\n",
    "To get unique values for given columns in the catalog, intake-esm provides a\n",
    "{py:meth}`~intake_esm.core.esm_datastore.unique` method:\n",
    "\n",
    "Let's query the data catalog to see what models(`source_id`), experiments\n",
    "(`experiment_id`) and temporal frequencies (`table_id`) are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831372f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = cat.unique()\n",
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e983ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique['source_id'].sort()\n",
    "unique['source_id'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fed967",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = unique['experiment_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments.sort()\n",
    "experiments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165f8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique['table_id'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76178cd6",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Q1: find a low and a high climate sensitivity model\n",
    "\n",
    "\n",
    "For the low sensitivity, I'll use MIROC6, ECS = 2.6 K/doubling, for the high sensitivity,  CESM2, ECS = 5.15 K/doubling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a857bed",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Q2: cloud fraction\n",
    "\n",
    "Check the variable list at [https://pcmdi.llnl.gov/mips/cmip3/variableList.html#overview](https://pcmdi.llnl.gov/mips/cmip3/variableList.html#overview)\n",
    "\n",
    "What is the difference between **cl** and **clt**?  What table do they belong to?\n",
    "\n",
    "These are both monthly averaged atmospheric variables in the table Amon -- clt is averaged over height and is 3 dimensional\n",
    "(time, lon, lat) while cl is four dimensional (time, height, lon, lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073e1e75",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Q3 Grab one realization from each of your two models for two scenarios: historical and ssp585\n",
    "\n",
    "First we need to find the available realizations.  To do this, get every realization by\n",
    "leaving \"member_id\" unspecified. Note that there are 50 realization for the historical runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f67df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_subset = cat.search(\n",
    "    experiment_id=[\"historical\",\"ssp585\"],\n",
    "    table_id=[\"Amon\",\"fx\"],\n",
    "    source_id = [\"CESM2\",\"MIROC6\"],\n",
    "    variable_id=[\"clt\",\"areacella\"],\n",
    "    grid_label=\"gn\")\n",
    "cat_subset.df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1688f2c1",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now sort by member_id to get a common realization -- everyone has member_id = \"r10i1p1f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6239f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_subset.search(experiment_id = [\"historical\",\"ssp585\"],source_id=[\"CESM2\",\"MIROC6\"]).df.sort_values(\"member_id\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f8e7ed",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "So grab these four realizations, plus the areacella weights for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2312fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_realization = cat_subset.search(experiment_id = [\"historical\",\"ssp585\"],source_id=[\"CESM2\",\"MIROC6\"],member_id = \"r10i1p1f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21dba7",
   "metadata": {
    "user_expressions": []
   },
   "outputs": [],
   "source": [
    "single_realization.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd9bd0",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "(sec:lazy)=\n",
    "## lazy-loading datasets by row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac38d35",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We can use [xarray.open_dataset](https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html) to read in the metadata from a google store zstore url.  This can take a few seconds, but it's \"lazy\" in the\n",
    "sense that no data is read until some calculation (a plot, average, etc.) is conducted.  If you want to force a data read,\n",
    "use [xarray.load_dataset](https://docs.xarray.dev/en/stable/generated/xarray.load_dataset.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179dddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get aeracella for miroc6 historical r10i1p1f1\n",
    "ds = xarray.open_zarr(single_realization.df.iloc[0]['zstore'])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ecddd0",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Load datasets using `to_dataset_dict()`\n",
    "\n",
    "Intake-esm implements convenience utilities for loading the query results into\n",
    "higher level xarray datasets. The logic for merging/concatenating the query\n",
    "results into higher level xarray datasets is provided in the input JSON file and\n",
    "is available under `.aggregation_info` property of the catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a536c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.esmcat.aggregation_control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee2ea9",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The purpose of aggregation control is to combine variable datasets from the same table_id into a single dataset if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1618f74b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Get the 8 datasets (this is a lazy operation)\n",
    "\n",
    "The following command reads in the consolidated metadata, but doesn't grab any actual data.\n",
    "The data reads will occur later when  you make select operations.\n",
    "\n",
    "The advantage of using `to_dataset_dict` over the raw `open_zarr` is that this this produces a dictionary with\n",
    "keys that mark the institution, model, scenario, table an grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dict = single_realization.to_dataset_dict(\n",
    "        xarray_open_kwargs={\"consolidated\": True, \"decode_times\": True, \"use_cftime\": True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb1bc72",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Keep track of the `areacella` for each dataset\n",
    "\n",
    "To keep track of the variables and weights, it helps to have the keys in two sorted lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f9fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_keys = [key for key in dset_dict.keys() if key.find('fx') > -1]\n",
    "cl_keys = [key for key in dset_dict.keys() if key.find('Amon') > -1]\n",
    "fx_keys.sort()\n",
    "cl_keys.sort()\n",
    "fx_keys, cl_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ddef19",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "(sec:isel)=\n",
    "## limiting the dataset to the Sourthern Ocean without reading the data\n",
    "\n",
    "To fetch the minimum amount of data, you can use [xarray.isel](https://docs.xarray.dev/en/stable/user-guide/indexing.html#vectorized-indexing) to\n",
    "describe a slice that will be applied when a calculation is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa311a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_so(the_ds):\n",
    "    \"\"\"\n",
    "    slice the dataset for latitudes south of -30 degrees\n",
    "    \"\"\"\n",
    "    #\n",
    "    # logical index of all latitudes south of -30\n",
    "    #\n",
    "    hit = the_ds.lat < -30\n",
    "    so_slice = the_ds.isel(indexers = {'lat':hit})\n",
    "    return so_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eded8b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "This reduces the latitude dimension size from 128 to 43, so your file size is 1/3 of the full dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c247332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, ds in dset_dict.items():\n",
    "    dset_dict[key]=get_so(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fbd313",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "(sec:checkpoint)=\n",
    "## saving a checkpoint\n",
    "\n",
    "You might want to load the dataset into memory (be careful) and save to disk for future acesss.  You\n",
    "can preview how much space you'll need with [xarray.dataset.nbytes](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.nbytes.html)\n",
    "\n",
    "The cell below shows how to checkpoint the datasets to disk and restore from disk back to a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d1048",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "```python\n",
    "readit = True\n",
    "write_files = False\n",
    "out_dir = home /\"cmip6_files\"\n",
    "out_dir.mkdir(exist_ok=True,parents=True)\n",
    "filenames=['cesm2_ssp585.nc','cesm2_historicial.nc','miroc6_ssp585.nc','miroc6_historical.nc']\n",
    "dataset_names =['ScenarioMIP.NCAR.CESM2.ssp585.Amon.gn',\n",
    "     'CMIP.NCAR.CESM2.historical.Amon.gn',\n",
    "     'ScenarioMIP.MIROC.MIROC6.ssp585.Amon.gn',\n",
    "     'CMIP.MIROC.MIROC6.historical.Amon.gn']\n",
    "if readit:\n",
    "    #\n",
    "    # download over the internet\n",
    "    #\n",
    "    dset_dict = single_realization.to_dataset_dict(\n",
    "        xarray_open_kwargs={\"consolidated\": True, \"decode_times\": True, \"use_cftime\": True}\n",
    "    )\n",
    "    if write_files:\n",
    "        for ds, filename in zip(dset_dict.values(),filenames):\n",
    "            out_file = out_dir / filename\n",
    "            print(f\"writing {out_file}\")\n",
    "            ds.to_netcdf(out_file)\n",
    "else:\n",
    "    #\n",
    "    # read from a diskfile\n",
    "    #\n",
    "    dset_dict = dict()\n",
    "    dset_dict[dataset_name]=xr.open_dataset(the_file)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3510c",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "(sec:weighted)=\n",
    "## Find the area weighted cloud fraction for 1 dataset over the southern ocean\n",
    "\n",
    "Note that I don't need a long variable name that includes all the metadata,\n",
    "I can get that information at any point from the variable itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dset_dict['ScenarioMIP.NCAR.CESM2.ssp585.Amon.gn']\n",
    "weights = dset_dict['ScenarioMIP.NCAR.CESM2.ssp585.fx.gn']['areacella']\n",
    "print(f\"{ds.experiment_id=}, {ds.member_id.data[0]=}, {ds.source_id=}\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f10632",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### The areacella weights\n",
    "\n",
    "We're on a sphere, so every latitude band is going to have grid cells with different areas.  The grid size\n",
    "will vary between models, so it's important to do weighted averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50325f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584733ce",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### squeeze out the unit dimensions\n",
    "\n",
    "Our variables come indexed over member id and starting year, but since we only have 1 member_id and 1 starting year, we\n",
    "can squeeze those out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1671ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['clt'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2491e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.squeeze()\n",
    "weights = weights.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2e61f5",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### use the weighted method to apply weights to the average\n",
    "\n",
    "The [dataArray.weighted](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.weighted.html) method makes a dataArray \"weight aware\".\n",
    "You want to make sure you copy this weighted dataArray to a new variable, because it wipes out all of the dataArray metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d353c0f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Add weights to the dataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d829ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clt_so  = ds['clt']\n",
    "clt_so =  ds['clt'].weighted(weights)\n",
    "clt_so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af1882",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Take the zonal mean\n",
    "\n",
    "Note that this is the first statement that actually fetches variable values from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b892742",
   "metadata": {
    "user_expressions": []
   },
   "outputs": [],
   "source": [
    "clt_so_zonal = clt_so.mean(dim=[\"lon\",\"lat\"])\n",
    "clt_so_zonal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ab298",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Smooth the data with a rolling window\n",
    "\n",
    "[https://docs.xarray.dev/en/stable/user-guide/computation.html#rolling-window-operations](https://docs.xarray.dev/en/stable/user-guide/computation.html#rolling-window-operations)\n",
    "\n",
    "In the cell below I use a 5-year running mean to remove shorter-timescale flucuations. Note how\n",
    "I include the metadata in the title for a legend entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df893b",
   "metadata": {
    "user_expressions": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "rolling = clt_so_zonal.rolling(time=60)\n",
    "clt_mean = rolling.mean()\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "clt_mean.plot(ax=ax,label = f\"{ds.experiment_id}, {ds.source_id}\")\n",
    "ax.grid(True)\n",
    "ax.set(title=\"southern ocean cloud fraction\",\n",
    "    xlabel=\"time (years)\", ylabel = \"cloud fraction (percent)\")\n",
    "fig.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0dc21d",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## now add miroc6\n",
    "\n",
    "Make a function so we don't need to copy/paste sells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_average(ds, weights):\n",
    "    ds = ds.squeeze()\n",
    "    weights = weights.squeeze()\n",
    "    clt_so_weighted = ds['clt'].weighted(weights)\n",
    "    clt_so_zonal = clt_so_weighted.mean(dim=[\"lon\",\"lat\"])\n",
    "    rolling = clt_so_zonal.rolling(time=60)\n",
    "    clt_mean = rolling.mean()\n",
    "    return clt_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed5a13d",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## add the miroc 5 year rolling mean to the plot\n",
    "\n",
    "looks like, for these realizations, there's a pretty dramatic difference between the hot and cold models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae9670",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_miroc =dset_dict['ScenarioMIP.MIROC.MIROC6.ssp585.Amon.gn']\n",
    "weights_miroc = dset_dict['ScenarioMIP.MIROC.MIROC6.ssp585.fx.gn']['areacella']\n",
    "miroc6_clt_mean = make_average(ds_miroc, weights_miroc)\n",
    "miroc6_clt_mean.plot(ax=ax,label = f\"{ds.source_id}, {ds.source_id}\")\n",
    "ax.set(title = (f\"Southern ocean cloud fraction comparison: \"\n",
    "                f\"{ds.source_id}:{ds.experiment_id} vs \"\n",
    "                f\"{ds_miroc.source_id}:{ds_miroc.experiment_id}\"),\n",
    "       xlabel = \"time (year)\", ylabel = \"cloud faction (percent)\")\n",
    "fig.legend()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0f0898",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Historical runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebdb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_miroc =dset_dict['CMIP.MIROC.MIROC6.historical.Amon.gn']\n",
    "weights_miroc = dset_dict['CMIP.MIROC.MIROC6.historical.fx.gn']['areacella']\n",
    "miroc6_clt_mean = make_average(ds_miroc, weights_miroc)\n",
    "miroc6_clt_mean.plot(ax=ax,label = f\"{ds.source_id}, {ds.source_id}\")\n",
    "ds_cesm =dset_dict['CMIP.NCAR.CESM2.historical.Amon.gn']\n",
    "weights_cesm = dset_dict['CMIP.NCAR.CESM2.historical.fx.gn']['areacella']\n",
    "cesm_clt_mean = make_average(ds_cesm, weights_cesm)\n",
    "cesm_clt_mean.plot(ax=ax,label = f\"{ds.source_id}, {ds.source_id}\")\n",
    "ax.set(title = (f\"Southern ocean cloud fraction comparison: \"\n",
    "                f\"{ds.source_id}:{ds.experiment_id} vs \"\n",
    "                f\"{ds_miroc.source_id}:{ds_miroc.experiment_id}\"),\n",
    "       xlabel = \"time (year)\", ylabel = \"cloud faction (percent)\")\n",
    "fig.legend()\n",
    "display(fig)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   43,
   48,
   56,
   62,
   67,
   73,
   77,
   101,
   106,
   111,
   115,
   120,
   124,
   131,
   142,
   149,
   159,
   163,
   167,
   171,
   175,
   181,
   186,
   192,
   198,
   207,
   211,
   215,
   225,
   231,
   235,
   241,
   249,
   257,
   270,
   274,
   279,
   289,
   321,
   329,
   336,
   343,
   347,
   354,
   358,
   363,
   371,
   375,
   381,
   387,
   394,
   403,
   417,
   423,
   434,
   440,
   453,
   457,
   461
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}